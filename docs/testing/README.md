# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã

Freqtrade Multi-Bot System –∏–º–µ–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â—É—é –∫–∞—á–µ—Å—Ç–≤–æ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```
tests/
‚îú‚îÄ‚îÄ conftest.py           # –û–±—â–∏–µ —Ñ–∏–∫—Å—Ç—É—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚îú‚îÄ‚îÄ test_api.py          # REST API —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ unit/                # Unit —Ç–µ—Å—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ test_bot_service.py
‚îÇ   ‚îú‚îÄ‚îÄ test_trading_gateway.py
‚îÇ   ‚îî‚îÄ‚îÄ test_strategy_analysis_service.py
‚îú‚îÄ‚îÄ integration/         # Integration —Ç–µ—Å—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ test_command_cycle.py
‚îú‚îÄ‚îÄ e2e/                 # End-to-end —Ç–µ—Å—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ test_bot_lifecycle.py
‚îî‚îÄ‚îÄ performance/         # Performance —Ç–µ—Å—Ç—ã
    ‚îî‚îÄ‚îÄ test_api_performance.py
```

## –¢–∏–ø—ã —Ç–µ—Å—Ç–æ–≤

### üîç Unit Tests
**–¶–µ–ª—å:** –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –∏–∑–æ–ª—è—Ü–∏–∏
- **Framework:** pytest
- **Coverage:** 80%+ –¥–ª—è critical paths
- **Mocking:** unittest.mock –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**–ü—Ä–∏–º–µ—Ä—ã:**
- BotService CRUD –æ–ø–µ—Ä–∞—Ü–∏–∏
- Strategy validation
- Trading Gateway command processing

### üîó Integration Tests
**–¶–µ–ª—å:** –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
- **Services:** Redis, Database, API
- **Communication:** HTTP, WebSocket, Redis Streams
- **Real dependencies:** –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π mocking

**–ü—Ä–∏–º–µ—Ä—ã:**
- API endpoints —Å —Ä–µ–∞–ª—å–Ω–æ–π –ë–î
- Redis Streams event processing
- Command cycle: API ‚Üí Redis ‚Üí Processing

### üåê E2E Tests
**–¶–µ–ª—å:** –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
- **Full stack:** Frontend ‚Üí API ‚Üí Database ‚Üí External services
- **Real environment:** –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—â–µ–Ω—ã
- **User workflows:** Registration ‚Üí Bot creation ‚Üí Trading

**–ü—Ä–∏–º–µ—Ä—ã:**
- Complete bot lifecycle
- Concurrent operations
- Error handling and recovery

### ‚ö° Performance Tests
**–¶–µ–ª—å:** –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π
- **Metrics:** Response time, throughput, memory usage
- **Load testing:** Concurrent users, sustained load
- **Benchmarks:** API performance, database queries

**–ü—Ä–∏–º–µ—Ä—ã:**
- API response times (<100ms for health, <500ms for operations)
- Concurrent user load (10+ simultaneous users)
- Memory usage under load (<50MB increase)

## –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

### –í—Å–µ —Ç–µ—Å—Ç—ã
```bash
pytest
```

### –ü–æ —Ç–∏–ø–∞–º
```bash
pytest tests/unit/          # Unit —Ç–µ—Å—Ç—ã
pytest tests/integration/   # Integration —Ç–µ—Å—Ç—ã
pytest tests/e2e/           # E2E —Ç–µ—Å—Ç—ã
pytest tests/performance/   # Performance —Ç–µ—Å—Ç—ã
```

### –° –æ–ø—Ü–∏—è–º–∏
```bash
pytest -v                    # Verbose output
pytest --cov=. --cov-report=html  # Coverage report
pytest --lf                  # Run only failed tests
pytest -k "test_bot"         # Run tests matching pattern
pytest --durations=10        # Show slowest tests
```

## Code Coverage

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
- **Unit tests:** 80%+ coverage
- **Integration tests:** 70%+ coverage
- **Critical paths:** 90%+ coverage

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ
```bash
pytest --cov=management_server --cov=trading_gateway --cov-report=html
# –û—Ç—á–µ—Ç –≤ htmlcov/index.html
```

### –ò—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è
```ini
# .coveragerc
[run]
omit =
    */tests/*
    */venv/*
    */migrations/*
    */__pycache__/*
```

## Test Fixtures

### conftest.py
```python
@pytest.fixture
async def app_client():
    """FastAPI test client"""
    from httpx import AsyncClient
    # Setup test client

@pytest.fixture
async def auth_headers(app_client):
    """JWT authentication headers"""
    # Login and return headers

@pytest.fixture
async def test_user(app_client):
    """Create test user"""
    # Register and return user data
```

### Database fixtures
```python
@pytest.fixture(autouse=True)
async def setup_test_database():
    """Clean database before each test"""
    # Setup test DB
    yield
    # Cleanup after test
```

## Best Practices

### 1. Test Organization
```python
class TestBotService:
    """Test cases for BotService"""

    @pytest.mark.asyncio
    async def test_create_bot_success(self, service):
        # Arrange
        # Act
        # Assert
```

### 2. Naming Conventions
- `test_should_create_bot_when_valid_data`
- `test_should_fail_when_invalid_strategy`
- `test_performance_under_concurrent_load`

### 3. Assertions
```python
# Good
assert result.status_code == 201
assert "bot_id" in result.json()

# Better
assert result.status_code == HTTPStatus.CREATED
assert result.json()["name"] == expected_name
```

### 4. Mocking Strategy
```python
# Mock external dependencies
with patch('redis.Redis') as mock_redis:
    mock_redis.return_value.ping.return_value = True
    # Test code
```

## CI/CD Integration

### GitHub Actions
```yaml
- name: Run tests
  run: |
    pytest --cov=. --cov-report=xml
- name: Upload coverage
  uses: codecov/codecov-action@v3
```

### Pre-commit Hooks
```yaml
repos:
  - repo: local
    hooks:
      - id: pytest
        name: pytest
        entry: pytest
        language: system
```

## Performance Benchmarks

### API Response Times
- **Health check:** <100ms
- **Bot list:** <200ms
- **Bot create:** <500ms
- **Bot start/stop:** <300ms

### Concurrent Load
- **10 users:** <1s average response
- **50 users:** <2s average response
- **Memory increase:** <50MB

### Database Queries
- **Simple select:** <50ms
- **Complex query:** <200ms
- **Bulk operations:** <1000ms

## Debugging Tests

### Verbose Output
```bash
pytest -v -s --tb=long
```

### PDB Debugging
```python
import pdb; pdb.set_trace()
```

### Logging in Tests
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Test Data Management

### Factory Pattern
```python
def create_test_bot(**overrides):
    defaults = {
        "name": "test_bot",
        "strategy_name": "TestStrategy",
        "exchange": "binance"
    }
    return {**defaults, **overrides}
```

### Cleanup Strategy
```python
@pytest.fixture(autouse=True)
async def cleanup():
    yield
    # Cleanup test data
    await db.execute("DELETE FROM bots WHERE name LIKE 'test_%'")
```

## Continuous Testing

### Test-Driven Development
1. Write failing test
2. Implement feature
3. Test passes
4. Refactor
5. Test still passes

### Regression Testing
- Run full test suite before releases
- Automated nightly runs
- Performance regression detection

## Troubleshooting

### Common Issues

**Tests fail randomly:**
- Race conditions in async code
- Database state not cleaned properly
- External service dependencies

**Slow tests:**
- Too many database operations
- Inefficient queries
- Heavy mocking

**Flaky tests:**
- Time-dependent logic
- Network timeouts
- Resource contention

### Solutions

**For async issues:**
```python
@pytest.mark.asyncio
async def test_async_function():
    await asyncio.sleep(0.1)  # Allow async operations
```

**For database cleanup:**
```python
@pytest.fixture(autouse=True)
async def clean_db(db_session):
    await db_session.execute("TRUNCATE TABLE bots CASCADE")
    await db_session.commit()
```

**For performance:**
```python
@pytest.mark.slow
def test_slow_operation():
    # Mark slow tests
    pass
```